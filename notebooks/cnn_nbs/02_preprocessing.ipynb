{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data loading & inspection:** \n",
    "Load the dataset and check for missig values and duplicates to ensure data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity     0\n",
      "title       18\n",
      "text         0\n",
      "dtype: int64\n",
      "Duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = \"../data/raw/amazon_reviews/train.csv\"\n",
    "df = pd.read_csv(csv_path, header=None, names=[\"polarity\", \"title\", \"text\"], nrows=200000)\n",
    "\n",
    "# check missing values and duplicates\n",
    "print(df.isnull().sum())\n",
    "print(\"Duplicates:\", df.duplicated().sum())\n",
    "\n",
    "# drop rows that has missing values\n",
    "df.dropna(subset=[\"title\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text cleaning:**\n",
    "The title and text columns were combined into a single full_review column. This provides a richer context for our sentiment analysis model(s).\n",
    "All text was converted to lowercase to standardize the data and reduce vocabulary size.\n",
    "We removed puncutation, numbers and extra whitespace using regular expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"full_review\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # removes special characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # removes extra whitespace\n",
    "    return text\n",
    "\n",
    "df[\"cleaned_review\"] = df[\"full_review\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text normalization:**\n",
    "Common english words were removed using NLTK's stopword list to focus on more meaningful words.\n",
    "We used Keras' Tokenizer to convert the cleaned text into sequences of integers. An OOV token was specified to handle words not seen during training.\n",
    "Sequences were padded to an appropriate length deemed by us, based on the histogram analysis from the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"cleaned_review\"] = df[\"cleaned_review\"].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "max_words = 5000\n",
    "maxlen = 70\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"cleaned_review\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"cleaned_review\"])\n",
    "\n",
    "# maxlen is a hyperparameter that can be tuned. we've set this to 60, based on the histogram created in notebook 1. \n",
    "# the histogram shows that the majority of reviews are below 60 words. we can adjust this in the future\n",
    "padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to ../data/processed/amazon_reviews_processed.csv\n"
     ]
    }
   ],
   "source": [
    "processed_path = \"../data/processed/amazon_reviews_processed.csv\"\n",
    "\n",
    "df.to_csv(processed_path, index=False)\n",
    "print(f\"Preprocessed data saved to {processed_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
